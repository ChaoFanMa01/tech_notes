{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boltzmann machines were originally introduced as a general “connectionist” approach to learning arbitrary probability distributions over binary vectors. We define the Boltzmann machine over a $d$-dimensional binary random vector $\\mathbf{x}\\in\\{0,1\\}^d$. The Boltzmann machine is an energy-based model (<font color=\"red\">什么是energy-based model?在书[3]中16.2.4节后续要去查.</font>), meaning we define the joint probability distribution using an energy function:\n",
    "$$\n",
    "P(\\mathbf{x})=\\frac{\\exp(-E(\\mathbf{x}))}{Z},\\ \\ \\ \\ \\ \\ \\ \\ \\ (20.1)\n",
    "$$\n",
    "where $E(\\mathbf{x})$ is the energy function and $Z$ is the partition function that ensures that $\\sum\\limits_{\\mathbf{x}}P(\\mathbf{x})=1$. The energy function of the Boltzmann machine is given by\n",
    "$$\n",
    "E(\\mathbf{x})=-\\mathbf{x}^T\\mathbf{U}\\mathbf{x}-\\mathbf{b}^T\\mathbf{x},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.2)\n",
    "$$\n",
    "where $\\mathbf{U}$ is the weight matrix of model parameters and $\\mathbf{b}$ is the vector of bias parameters.\n",
    "\n",
    ">能量$E$越小，对应状态的概率越大。$Z$是配分函数，用作归一化。利用基于能量的模型的原因是这样的，对于一个给定的数据集，如果不知道其潜在的分布形式，那是非常难学习的，似然函数都写不出来。比如如果知道是高斯分布或者多项分布，那可以用最大化似然函数来学出需要学习的对应参数，但是如果分布的可能形式都不知道，这个方法就行不通。而统计力学的结论表明，任何概率分布都可以转变成基于能量的模型，所以利用基于能量的模型的这个形式，是一种学习概率分布的通法。[4]\n",
    "\n",
    "In the general setting of the Boltzmann machine, we are given a set of training examples, each of which are $n$-dimensional. Eq. 20.1 describes the joint probability distribution over the observed variables.\n",
    "\n",
    "The Boltzmann machine becomes more powerful when not all the variables are observed. In this case, the non-observed variables, or **latent** variables, can act similarly to hidden units in a multi-layer perceptron and model higher-order interactions among the visible units.\n",
    "\n",
    "Formally, we decompose the units $\\mathbf{x}$ into two subsets: the visible units $\\mathbf{v}$ and the latent (or hidden) units $\\mathbf{h}$. The energy function becomse (<font color=\"red\">这个式子怎么得到的?</font>)\n",
    "$$\n",
    "E(\\mathbf{v},\\mathbf{h})=-\\mathbf{v}^T\\mathbf{R}\\mathbf{v}-\\mathbf{v}^T\\mathbf{W}\\mathbf{h}-\\mathbf{h}^T\\mathbf{S}\\mathbf{h}-\\mathbf{b}^T\\mathbf{v}-\\mathbf{c}^T\\mathbf{h}.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.3)\n",
    "$$\n",
    "\n",
    "这是因为Boltzmann machine的结构如下图所示, 不仅可以与其它变量(节点)连接还可以与自身连接\n",
    "\n",
    "<img src=\"./figs/boltzmann_1.png\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RBMs are undirected probabilistic graphical models containing a layer of observable variables and a single layer of latent variables. RBMs may be stacked (one on top of the other) to form deeper models. \n",
    "\n",
    " The RBM is an energy-based model with the joint probability distribution specified by tis energy function\n",
    " $$\n",
    "P(\\mathbf{v}=\\boldsymbol{v}, \\mathbf{h}=\\boldsymbol{h})=\\frac{1}{Z}\\exp(-E(\\boldsymbol{v},\\boldsymbol{h})).\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.4)\n",
    " $$\n",
    "\n",
    " The energy function an RBM is given by (<font color=\"red\">这个公式怎么又和刚才的不一样了?</font>)\n",
    "$$\n",
    "E(\\boldsymbol{v},\\boldsymbol{h})=-\\mathbf{b}^T\\boldsymbol{v}-\\boldsymbol{c}^T\\boldsymbol{h}-\\boldsymbol{v}^T\\mathbf{W}\\boldsymbol{h}, \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ (20.5)\n",
    "$$\n",
    "\n",
    "上面问题的主要原因还是源于RBM的结构. 一个RBM的结构示例如下所示, 可以看到RBM中一个变量(节点)只能与相邻层的节点连接, 不能与自身连接.\n",
    "\n",
    "<img src=\"./figs/boltzmann_2.png\" width=\"300px\" />\n",
    "\n",
    " and $Z$ is the normalizing constant known as the partition function\n",
    " $$\n",
    "Z=\\sum\\limits_{\\boldsymbol{v}}\\sum\\limits_{\\boldsymbol{h}}\\exp(-E(\\boldsymbol{v},\\boldsymbol{h})).\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.6)\n",
    " $$\n",
    "\n",
    " Though $P(\\boldsymbol{v})$ is intractable, the bipartite graph structure of the RBM has the very special property that is conditional distributions $P(\\boldsymbol{h}|\\boldsymbol{v})$ and $P(\\boldsymbol{v}|\\boldsymbol{h})$ are factorial and relatively simple to compute and to sample from.\n",
    "\n",
    " Diriving the conditional distributions from the joint distribution is straightforward\n",
    " $$\n",
    "\\begin{split}\n",
    "P(\\boldsymbol{h}|\\boldsymbol{v})&=\\frac{P(\\boldsymbol{h}, \\boldsymbol{v})}{P(\\boldsymbol{v})}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.7)\\\\\n",
    "&=\\frac{1}{P(\\boldsymbol{v})}\\underbrace{\\frac{1}{Z}\\exp\\left(\\boldsymbol{b}^T\\boldsymbol{v}+\\boldsymbol{c}^T\\boldsymbol{h}+\\boldsymbol{v}^T\\boldsymbol{W}\\boldsymbol{h}\\right)}_{\\text{part I}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.8)\\\\\n",
    "&=\\frac{1}{Z'}\\exp\\left(\\boldsymbol{c}^T\\boldsymbol{h}+\\boldsymbol{v}^T\\boldsymbol{W}\\boldsymbol{h}\\right)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.9)\\\\\n",
    "&=\\frac{1}{Z'}\\exp\\left(\\sum\\limits_{j=1}^{n_h}c_jh_j+\\sum\\limits_{j=1}^{n_h}\\boldsymbol{v}^T\\boldsymbol{W}_{:,j}h_j\\right)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.10)\\\\\n",
    "&=\\frac{1}{Z'}\\prod\\limits_{j=1}^{n_h}\\exp\\left(c_jh_j+\\boldsymbol{v}^T\\boldsymbol{W}_{:,j}h_j\\right)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.11)\n",
    "\\end{split}\n",
    " $$\n",
    "\n",
    " 式(20.8)中的part I就是$P(\\boldsymbol{h},\\boldsymbol{v})$. 式(20.9)中的$Z'=P(\\boldsymbol{v})Z/\\exp(\\boldsymbol{b}^T\\boldsymbol{v})$.\n",
    "\n",
    " Since we are conditioning on the visible units $\\mathbf{v}$, we can treat these as constant with respoect to the distribution $P(\\mathbf{h}|\\mathbf{v})$ (<font color=\"red\">这句话没有看明白</font>). The factorial nature of the conditional $P(\\mathbf{h}|\\mathbf{v})$ follows immediately from our ability to write the joint probability over the vector $\\boldsymbol{h}$ as the product of (unnormalized) distributions over the individual elements, $h_j$. It is now a simple matter of normalizing the distribution over the individual binary $h_j$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(h_j|\\boldsymbol{v})&=\\frac{\\tilde{P}(h_j=1|\\boldsymbol{v})}{\\tilde{P}(h_j=0|\\boldsymbol{v})+\\tilde{P}(h_j=1|\\boldsymbol{v})}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.12)\\\\\n",
    "&=\\frac{\\exp(c_j+\\boldsymbol{v}^T\\boldsymbol{W}_{:,j})}{\\exp(0)+\\exp(c_j+\\boldsymbol{v}^T\\boldsymbol{W}_{:,j})}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.13)\\\\\n",
    "&=\\sigma(c_j+\\boldsymbol{v}^T\\boldsymbol{W}_{:,j}).\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.14)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "<font color=\"red\">只能推导到这一步了, 下面目前还没有想出来如何推导的.</font>\n",
    "\n",
    "We can now express the full conditional over the hidden layer as the factorial distribution\n",
    "$$\n",
    "P(\\boldsymbol{h}|\\boldsymbol{v})=\\prod\\limits_{j=1}^{n_h}\\sigma\\left((2\\boldsymbol{h}-1)\\odot (\\boldsymbol{c}+\\boldsymbol{W}^T\\boldsymbol{v})\\right)_j.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.15) \n",
    "$$\n",
    "\n",
    "A similar derivation will show that the other condition of interest to us, $P(\\boldsymbol{v}|\\boldsymbol{h})$, is also a factorial distribution\n",
    "$$\n",
    "P(\\boldsymbol{v}|\\boldsymbol{h})=\\prod\\limits_{i=1}^{n_v}\\sigma\\left((2\\boldsymbol{v}-1)\\odot (\\boldsymbol{b}+\\boldsymbol{W}^T\\boldsymbol{h})\\right)_i.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (20.16) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 暂时总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8], [9]看完基本上就对Boltzmann machine了解的比较清晰, 但是还需要EM, 随机场等相关知识.\n",
    "\n",
    "[7]中有RBM和DBN的具体实现.\n",
    "\n",
    "**RBM总结**\n",
    "\n",
    "给定一个RBM, 优化目标为最大化似然值$\\ln\\mathcal{L}(\\boldsymbol{\\theta}|S)$. 更进一步maximizing the log-likelihood corresponds to minimizing the KL-divergence. 一般采用梯度下降算法优化, 其每步更新方法如下\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{(t+1)}=\\boldsymbol{\\theta}^{(t)}+\\eta \\frac{\\partial}{\\partial\\boldsymbol{\\theta}^{(t)}}\\left(\\sum\\limits_{i=1}^N\\ln\\mathcal{L}(\\boldsymbol{\\theta}^{(t)})|\\boldsymbol{x}_i\\right)-\\lambda\\boldsymbol{\\theta}^{(t)}+\\nu\\Delta\\boldsymbol{\\theta}^{(t-1)}.\n",
    "$$\n",
    "\n",
    "**binary RBM**\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{v},\\boldsymbol{h})=-\\sum\\limits_i a_iv_i-\\sum\\limits_{j} b_jh_j-\\sum\\limits_{i,j}v_ih_j\\omega_{ij}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(h_i=1|\\boldsymbol{v})=\\sigma\\left(\\sum\\limits_{j=1}^m\\omega_{ij}v_j+c_i\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(v_j=1|\\boldsymbol{h})=\\sigma\\left(\\sum\\limits_{j=1}^m\\omega_{ij}h_i+b_j\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln\\mathcal{L}(\\boldsymbol{\\theta}|\\boldsymbol{v})}{\\partial\\omega_{ij}}=p(h_i=1|\\boldsymbol{v})v_j-\\sum\\limits_{\\boldsymbol{v}}p(\\boldsymbol{v})p(h_i=1|\\boldsymbol{v})v_j.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln\\mathcal{L}(\\boldsymbol{\\theta}|\\boldsymbol{v})}{\\partial b_{j}}=v_j-\\sum\\limits_{\\boldsymbol{v}}p(\\boldsymbol{v})v_j.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln\\mathcal{L}(\\boldsymbol{\\theta}|\\boldsymbol{v})}{\\partial c_{i}}=p(h_i=1|\\boldsymbol{v})-\\sum\\limits_{\\boldsymbol{v}}p(h_i=1|\\boldsymbol{v})p(\\boldsymbol{v}).\n",
    "$$\n",
    "\n",
    "Binary RBM的CD-K算法如下所示.\n",
    "\n",
    "<img src=\"./figs/binary_cd_k.png\" width=\"600px\" />\n",
    "\n",
    "**real-valued RBM (Gaussian)**\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{v},\\boldsymbol{h})=\\sum\\limits_i\\frac{(v_i-a_i)^2}{2\\sigma_i^2}+\\sum\\limits_{j}\\frac{(h_j-b_j)^2}{2\\sigma_j^2}-\\sum\\limits_{i,j}\\frac{v_i}{\\sigma_i}\\frac{h_j}{\\sigma_j}\\omega_{ij}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(h_j|\\boldsymbol{v})\\sim \\mathcal{N}(\\mu_j,\\sigma_j),\\ \\mu_j=b_j+\\sigma_j\\sum\\limits_{i}\\frac{v_i}{\\sigma_j}\\omega_{ij}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(v_i|\\boldsymbol{h})\\sim\\mathcal{N}(\\mu_i,\\sigma_i),\\ \\mu_i=a_i+\\sigma_i\\sum\\limits_{j}\\frac{h_j}{\\sigma_i}\\omega_{ij}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta\\omega_{ij}=v_i^{(0)}h_j^{(0)}-v_i^{(k)}h_j^{(k)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta a_i=v_i^{(0)}-v_i^{(k)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta b_j=h_j^{(0)}-h_j^{(k)}\n",
    "$$\n",
    "\n",
    "Real-valued RBM的CD-K算法如下所示.\n",
    "\n",
    "<img src=\"./figs/real_valued_cd_k.png\" width=\"600px\"/>\n",
    "\n",
    "目前具体模型实现在`model/rbm.py`中, 测试脚本在`script/rbm_test.py`. 在训练的时候必须注意初始值设置, 如果像原论文那样把RBM模型参数`w`, `b`和`c`初始化为0, 模型是不能学习的, 应该像其它架构一样随机初始化参数. 另外, 初始化的值应该较小, 要不然容易梯度爆炸. 而且, binary-valued RBM和real-valued RBM的初始化不一样.\n",
    "\n",
    "<font color=\"red\">后续优化或重新实现的时候, 可以参考`sklearn`中的`BernoulliRBM`模型: https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/neural_network/_rbm.py#L26</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinton et al. [1] introduced a greedy layer-wise *unsupervised* learning algorithm for Beep Belief Networks (DBN), a generative model with many layers of hidden causal variables. Upper layers of a DBN are supposed to represent more \"abstract\" concept that explain the input observation $x$, whereas lower layers extract \"low-level features\" from $x$. They learn simpler concepts first, and build on them to learn more abstract concepts. This strategy, studied in detail here, has not yet been much exploited in machine learning. We hypothesize that three aspectsof this strategy are particularly important: first, pre-training one layer at a time in a  greedy way; second, using unsupervised learning at each layer in order to preserve information from the input; and finally, fine-tuning the whole network with respect to the ultimate criterion of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Belief Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be the input, and $\\mathbf{g}^i$ the hidden variables at layer $i$, with joint distribution\n",
    "$$\n",
    "P(x,\\mathbf{g}^1,\\mathbf{g}^2,\\dots,\\mathbf{g}^l)=P(x|\\mathbf{g}^1)P(\\mathbf{g}^1|\\mathbf{g}^2)\\cdots P(\\mathbf{g}^{l-2}|\\mathbf{g}^{l-1})P(\\mathbf{g}^{l-1},\\mathbf{g}^l),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] G. E. Hinton, S. Osindero, and Y. Teh, \"A Fast Learning Algorithm for Deep Belief Nets,\" *Neural Computation*, vol. 18, pp. 1527-1554, 2006.\n",
    "\n",
    "[2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \"Greedy Layer-Wise Training of Deep Networks,\" *in proc. NIPS'06*, 2006.\n",
    "\n",
    "[3] I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning,\" .\n",
    "\n",
    "[4] \"深度学习基础：Boltzmann Machines\", https://zhuanlan.zhihu.com/p/34201655.\n",
    "\n",
    "[5] \"受限玻尔兹曼机(RBM)与python在Tensorflow的实现\", https://blog.csdn.net/sinat_28371057/article/details/115795086.\n",
    "\n",
    "[6] \"Python 3深度置信网络(DBN)在Tensorflow中的实现MNIST手写数字识别\", https://blog.csdn.net/sinat_28371057/article/details/115794457.\n",
    "\n",
    "[7] https://github.com/zhuofupan/Pytorch-Deep-Neural-Networks/blob/master/fuzz/model/dbn.py\n",
    "\n",
    "[8] A. Fischer, and C. Igel, \"An Introduction to Restricted Boltzmann Machines,\" *LNCS*, vol. 7441, pp. 14-36, 2012.\n",
    "\n",
    "[9] Y. Wang, Z. Pan, X. Yuan, C. Yang, and W. Gui, \"A Novel Deep Learning based Fault Diagnosis Approach for Chemical Process with Extended Deep Belief Network,\" *ISA Trans.*, vol. 96, pp.457-467, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
